{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id_to_name = {\n",
    "    \"0\": \"air_conditioner\",\n",
    "    \"1\": \"car_horn\",\n",
    "    \"2\": \"children_playing\",\n",
    "    \"3\": \"dog_bark\",\n",
    "    \"4\": \"drilling\",\n",
    "    \"5\": \"engine_idling\",\n",
    "    \"6\": \"gun_shot\",\n",
    "    \"7\": \"jackhammer\",\n",
    "    \"8\": \"siren\",\n",
    "    \"9\": \"street_music\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../UrbanSound-Spectrogram/fold2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m fold_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Iterate through each file in the fold directory\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m# Extract classID from the filename\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         class_id \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../UrbanSound-Spectrogram/fold2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the base directory where the folds are located\n",
    "base_dir = \"../UrbanSound-Spectrogram\"\n",
    "\n",
    "# Define the output file\n",
    "output_file = \"urbansound_dataset.jsonl\"\n",
    "\n",
    "# Open the output file for writing\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    # Iterate through each fold directory\n",
    "    for fold in range(1, 11):\n",
    "        fold_dir = os.path.join(base_dir, f\"fold{fold}\")\n",
    "        # Iterate through each file in the fold directory\n",
    "        for filename in os.listdir(fold_dir):\n",
    "            if filename.endswith(\".png\"):\n",
    "                # Extract classID from the filename\n",
    "                class_id = filename.split(\"-\")[1]\n",
    "                # Map classID to class name\n",
    "                label = class_id_to_name.get(class_id, \"unknown\")\n",
    "                # Construct the full path to the image\n",
    "                image_path = os.path.join(fold_dir, filename)\n",
    "                # Create the data entry\n",
    "                data_entry = {\n",
    "                    \"image\": image_path,\n",
    "                    \"text\": \"Classify the sound in this spectrogram.\",\n",
    "                    \"label\": label\n",
    "                }\n",
    "                # Write the JSON line to the output file\n",
    "                outfile.write(json.dumps(data_entry) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[30522,  3751,   102]], device='cuda:0')\n",
      "electric\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVisualQuestionAnswering\n",
    "from PIL import Image\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = AutoModelForVisualQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\", torch_dtype=\"auto\").to(\"cuda\", torch.float16)\n",
    "\n",
    "raw_image = Image.open(\"../UrbanSound-Spectrogram/fold1/98223-7-2-0.png\").convert('RGB')\n",
    "\n",
    "question = \"Classify the sound in this spectrogram ? Is it an air conditioner, car horn, children playing, dog bark, drilling, engine idling, gun shot, jackhammer, siren or street music ?\"\n",
    "inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "\n",
    "output = model.generate(**inputs)\n",
    "print(output)\n",
    "print(processor.decode(output[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 698\n",
      "Validation examples: 175\n",
      "trainable params: 1,179,648 || all params: 385,852,220 || trainable%: 0.3057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 698/698 [07:07<00:00,  1.63 examples/s]\n",
      "Map: 100%|██████████| 175/175 [01:47<00:00,  1.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/175 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4] at entry 0 and [3] at entry 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 201\u001b[0m\n\u001b[1;32m    192\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m    193\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    194\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer\n\u001b[1;32m    198\u001b[0m )\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[1;32m    204\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 114\u001b[0m, in \u001b[0;36mCustomTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    112\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m    115\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.miniconda3/envs/vlm/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/.miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:171\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m--> 171\u001b[0m         {\n\u001b[1;32m    172\u001b[0m             key: collate(\n\u001b[1;32m    173\u001b[0m                 [d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map\n\u001b[1;32m    174\u001b[0m             )\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    176\u001b[0m         }\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    171\u001b[0m         {\n\u001b[0;32m--> 172\u001b[0m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    176\u001b[0m         }\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/.miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4] at entry 0 and [3] at entry 3"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVisualQuestionAnswering, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('json', data_files='urbansound_dataset.jsonl', split='train')\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['test']\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(val_dataset)}\")\n",
    "\n",
    "# Load the processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = AutoModelForVisualQuestionAnswering.from_pretrained(\n",
    "    \"Salesforce/blip-vqa-base\", \n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Define LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # dimension of the low-rank matrices\n",
    "    lora_alpha=32,  # scaling factor\n",
    "    target_modules=[\"query\", \"\"],  # layers to apply LoRA\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.QUESTION_ANS\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Prints the percentage of trainable parameters\n",
    "\n",
    "# Define data preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    questions = [\"Classify the sound in this spectrogram.\"] * len(examples['image'])\n",
    "    images = [Image.open(image_path).convert('RGB') for image_path in examples['image']]\n",
    "    labels = examples['label']\n",
    "    \n",
    "    # Encode the inputs\n",
    "    inputs = processor(\n",
    "        images=images, \n",
    "        text=questions, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=\"max_length\",  # Use max_length padding\n",
    "        max_length=32,  # Set a fixed max length for input ids\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Encode the labels with fixed length\n",
    "    encoded_labels = processor.tokenizer(\n",
    "        labels, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=\"max_length\", \n",
    "        max_length=8,  # Set a fixed max length for labels\n",
    "        truncation=True\n",
    "    ).input_ids\n",
    "    \n",
    "    # Remove special tokens (BOS/EOS)\n",
    "    encoded_labels = encoded_labels[:, 1:-1] \n",
    "    \n",
    "    # Create attention mask for labels\n",
    "    label_attention_mask = (encoded_labels != 0).long()\n",
    "    \n",
    "    inputs[\"labels\"] = encoded_labels\n",
    "    inputs[\"label_attention_mask\"] = label_attention_mask\n",
    "    \n",
    "    return inputs\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Preprocess the datasets\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, batch_size=8)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True, batch_size=8)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['pixel_values', 'input_ids', 'attention_mask', 'labels', 'label_attention_mask'])\n",
    "val_dataset.set_format(type='torch', columns=['pixel_values', 'input_ids', 'attention_mask', 'labels', 'label_attention_mask'])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip-vqa-lora-urbansound\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    # save_strategy=\"epoch\",\n",
    "    # load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,  # Use mixed precision training\n",
    ")\n",
    "\n",
    "# Define custom trainer to handle the sequence generation\n",
    "class CustomTrainer:\n",
    "    def __init__(self, model, args, train_dataloader, val_dataloader, optimizer):\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        for epoch in range(int(self.args.num_train_epochs)):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.num_train_epochs}\")\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(self.train_dataloader)\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(\n",
    "                    pixel_values=batch[\"pixel_values\"],\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    labels=batch[\"labels\"]\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                progress_bar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            avg_loss = total_loss / len(self.train_dataloader)\n",
    "            print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Evaluation\n",
    "            eval_results = self.evaluate()\n",
    "            print(f\"Validation Loss: {eval_results['loss']:.4f}, Accuracy: {eval_results['accuracy']:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            torch.save(self.model.state_dict(), f\"{self.args.output_dir}/model_epoch_{epoch+1}.pt\")\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_dataloader, desc=\"Evaluating\"):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    pixel_values=batch[\"pixel_values\"],\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    labels=batch[\"labels\"]\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Generate predictions\n",
    "                generated_ids = self.model.generate(\n",
    "                    pixel_values=batch[\"pixel_values\"],\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_length=20\n",
    "                )\n",
    "                \n",
    "                pred_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                label_texts = processor.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "                \n",
    "                all_preds.extend(pred_texts)\n",
    "                all_labels.extend(label_texts)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = sum(1 for pred, label in zip(all_preds, all_labels) if pred.strip() == label.strip())\n",
    "        accuracy = correct / len(all_labels)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": total_loss / len(self.val_dataloader),\n",
    "            \"accuracy\": accuracy\n",
    "        }\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "# Create and run the trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "model.save_pretrained(f\"{training_args.output_dir}/final_model\")\n",
    "\n",
    "# Test the model on a sample image\n",
    "def test_model(image_path, question):\n",
    "    # Load the fine-tuned model\n",
    "    fine_tuned_model = AutoModelForVisualQuestionAnswering.from_pretrained(\n",
    "        f\"{training_args.output_dir}/final_model\",\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    raw_image = Image.open(image_path).convert('RGB')\n",
    "    inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "    \n",
    "    output = fine_tuned_model.generate(**inputs)\n",
    "    answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test the model on a sample\n",
    "sample_image = \"../UrbanSound-Spectrogram/fold1/98223-7-2-0.png\"\n",
    "question = \"Classify the sound in this spectrogram.\"\n",
    "prediction = test_model(sample_image, question)\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found attention module: text_encoder.encoder.layer.0.attention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.0.attention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.0.attention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.0.crossattention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.0.crossattention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.0.crossattention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.1.attention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.1.attention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.1.attention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.1.crossattention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.1.crossattention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.1.crossattention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.2.attention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.2.attention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.2.attention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.2.crossattention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.2.crossattention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.2.crossattention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.3.attention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.3.attention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.3.attention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.3.crossattention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.3.crossattention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.3.crossattention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.4.attention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.4.attention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.4.attention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.4.crossattention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.4.crossattention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.4.crossattention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.5.attention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.5.attention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.5.attention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.5.crossattention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.5.crossattention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.5.crossattention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.6.attention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.6.attention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.6.attention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.6.crossattention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.6.crossattention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.6.crossattention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.7.attention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.7.attention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.7.attention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.7.crossattention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.7.crossattention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.7.crossattention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.8.attention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.8.attention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.8.attention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.8.crossattention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.8.crossattention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.8.crossattention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.9.attention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.9.attention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.9.attention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.9.crossattention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.9.crossattention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.9.crossattention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.10.attention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.10.attention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.10.attention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.10.crossattention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.10.crossattention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.10.crossattention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.11.attention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.11.attention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.11.attention.self.value\n",
      "Found attention module: text_encoder.encoder.layer.11.crossattention.self.query\n",
      "Found attention module: text_encoder.encoder.layer.11.crossattention.self.key\n",
      "Found attention module: text_encoder.encoder.layer.11.crossattention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.0.attention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.0.attention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.0.attention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.0.crossattention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.0.crossattention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.0.crossattention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.1.attention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.1.attention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.1.attention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.1.crossattention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.1.crossattention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.1.crossattention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.2.attention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.2.attention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.2.attention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.2.crossattention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.2.crossattention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.2.crossattention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.3.attention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.3.attention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.3.attention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.3.crossattention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.3.crossattention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.3.crossattention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.4.attention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.4.attention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.4.attention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.4.crossattention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.4.crossattention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.4.crossattention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.5.attention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.5.attention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.5.attention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.5.crossattention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.5.crossattention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.5.crossattention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.6.attention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.6.attention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.6.attention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.6.crossattention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.6.crossattention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.6.crossattention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.7.attention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.7.attention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.7.attention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.7.crossattention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.7.crossattention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.7.crossattention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.8.attention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.8.attention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.8.attention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.8.crossattention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.8.crossattention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.8.crossattention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.9.attention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.9.attention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.9.attention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.9.crossattention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.9.crossattention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.9.crossattention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.10.attention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.10.attention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.10.attention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.10.crossattention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.10.crossattention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.10.crossattention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.11.attention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.11.attention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.11.attention.self.value\n",
      "Found attention module: text_decoder.bert.encoder.layer.11.crossattention.self.query\n",
      "Found attention module: text_decoder.bert.encoder.layer.11.crossattention.self.key\n",
      "Found attention module: text_decoder.bert.encoder.layer.11.crossattention.self.value\n",
      "\n",
      "Sample of module names (first 20):\n",
      "\n",
      "vision_model\n",
      "vision_model.embeddings\n",
      "vision_model.embeddings.patch_embedding\n",
      "vision_model.encoder\n",
      "vision_model.encoder.layers\n",
      "vision_model.encoder.layers.0\n",
      "vision_model.encoder.layers.0.self_attn\n",
      "vision_model.encoder.layers.0.self_attn.dropout\n",
      "vision_model.encoder.layers.0.self_attn.qkv\n",
      "vision_model.encoder.layers.0.self_attn.projection\n",
      "vision_model.encoder.layers.0.layer_norm1\n",
      "vision_model.encoder.layers.0.mlp\n",
      "vision_model.encoder.layers.0.mlp.activation_fn\n",
      "vision_model.encoder.layers.0.mlp.fc1\n",
      "vision_model.encoder.layers.0.mlp.fc2\n",
      "vision_model.encoder.layers.0.layer_norm2\n",
      "vision_model.encoder.layers.1\n",
      "vision_model.encoder.layers.1.self_attn\n",
      "vision_model.encoder.layers.1.self_attn.dropout\n",
      "trainable params: 2,359,296 || all params: 387,031,868 || trainable%: 0.6096\n"
     ]
    }
   ],
   "source": [
    "# Add this code to identify target modules in the model\n",
    "def find_target_modules(model):\n",
    "    \"\"\"Helper function to find potential target modules for LoRA.\"\"\"\n",
    "    target_modules = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if 'query' in name or 'key' in name or 'value' in name:\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                target_modules.add(parent_name)\n",
    "                print(f\"Found attention module: {name}\")\n",
    "    \n",
    "    # Also print some example module names to help identify patterns\n",
    "    print(\"\\nSample of module names (first 20):\")\n",
    "    for i, (name, _) in enumerate(model.named_modules()):\n",
    "        if i < 20:\n",
    "            print(name)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return target_modules\n",
    "\n",
    "# Check the model architecture to identify the correct target modules\n",
    "find_target_modules(model)\n",
    "\n",
    "# Now update the LoRA configuration with correct target modules\n",
    "# For BLIP models, the target modules are likely to be something like:\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # dimension of the low-rank matrices\n",
    "    lora_alpha=32,  # scaling factor\n",
    "    target_modules=[\"query\", \"value\"],  # Update these based on the output above\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
